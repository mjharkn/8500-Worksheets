lib---
title: 'Worksheet 6: Topic Modeling'
author: 'Mandolyn Harknesss'
date: '04-14-2025'
---

_This is the sixth in a series of worksheets for History 8500 at Clemson University. The goal of these worksheets is simple: practice, practice, practice. The worksheet introduces concepts and techniques and includes prompts for you to practice in this interactive document. When you are finished, you should change the author name (above), render your document to a pdf, and upload it to canvas. Don't forget to commit your changes as you go and push to github when you finish the worksheet._

Text analysis is an umbrella for a number of different methodologies. Generally speaking, it involves taking a set (or corpus) of textual sources, turning them into data that a computer can understand, and then running calculations and algorithms using that data. Typically, at its most basic level, that involves the counting of words.

Topic modeling (TM) is one type of text analysis that is particularly useful for historians. 

TM takes collections or corpuses of documents and returns groups of "topics" from those documents. It is a form of unsupervised classification that finds groups of items that are probabilistically likely to co-occur. 

Latent Dirichlet allocation (LDA) is the most popular algorithm or method for topic modeling, although there are others. It assumes that each document has a mixture of topics and that each topic is a mixture of words. That means that topics overlap each other in terms of content rather than being confined to distinct and singular groups. 

To prepare a corpus for topic modeling, we'll do many of the same types of operations that we used last week to prepare a corpus for analysis. First we'll pre-process the data and then we'll create a document term matrix from our corpus using the `tm` (text mining) package. 

```{r}
library(tidytext)
library(tidyverse)
library(readtext)
library(tm)
library(topicmodels)
```

```{r}
download.file("https://github.com/regan008/8510-TextAnalysisData/blob/main/TheAmericanCity.zip?raw=true", "AmCity.zip")
unzip("AmCity.zip")
```

```{r}
# Metadata that includes info about each issue.
metadata <- read.csv("https://raw.githubusercontent.com/regan008/8510-TextAnalysisData/main/AmCityMetadata.csv")

meta <- as.data.frame(metadata)
meta$Filename <- paste("MB_", meta$Filename, sep="")
file_paths <- system.file("TheAmericanCity/")
ac_texts <- readtext(paste("TheAmericanCity/", "*.txt", sep=""))
ac_whole <- full_join(meta, ac_texts, by = c("filename" = "doc_id")) %>% as_tibble() 

tidy_ac <- ac_whole %>%
  unnest_tokens(word, text) %>% 
  filter(str_detect(word, "[a-z']$")) %>% 
  anti_join(stop_words)

tidy_ac <- tidy_ac %>% filter(!grepl('[0-9]', word))

```
The above code borrows from what we did last week. It pulls in the texts from the _The American City_ corpus, joins them together into a single data frame, and then turns then uses `unnest_tokens()` to tokenize the text and, finally, removes stop words. 

For topic modeling, we need a Document Term Matrix, or a DTM. Topic Modeling has the documents running down one side and the terms across the top. `Tidytext` provides a function for converting to and from DTMs. First, we need to create a document that has the doc_id, the word and the count of the number of times that word occurs. We can do that using `count()`.

```{r}
tidy_ac_words <- tidy_ac %>% count(filename, word)
```

Now we can use `cast_dtm()` to turn `tidy_mb_words` into a dtm. 

```{r}
ac.dtm <- tidy_ac_words %>% 
  count(filename, word) %>% 
  cast_dtm(filename, word, n)
```

If you run `class(mb.dtm)` in your console you will notice that it now has a class of "DocumentTermMatrix". 

Now that we have a dtm, we can create a topic model. For this, we'll use the topic models package and the `LDA()` function. Take a minute and read the documentation for `LDA()`.

There are two important options when running `LDA()`. The first is k which is the number of topics you want the model to generate. What number topics you generate is a decision that often takes some experimentation and depends on the size of your corpus. The American City corpus isn't that bigbut still has over 209k words. In this instance, because the corpus is so small we're going to start with a small number of topics. Going above 5 causes errors with this particular corpus. Later, when you work with a different corpus you should experiment with changing the number of topics from 10 to 20 to 30 to 50 to see how it changes your model.

The second important option when running `LDA()` is the seed option. You don't worry too much about what setting the seed does, but put simply - it ensures the output of the model is predictable and reproducible. Using the seed ensures that if you come back to your code later or someone else tries to run it, the model will return exactly the same results. 

Lets now train our model. This will take a few minutes: 
```{r}
ac.lda <- LDA(ac.dtm, k = 5, control = list(seed = 12345))
ac.lda
```

Now we have a LDA topic model that has 5 topics. There are two ways to look at this model: word-topic probabilities and document-topic probabilities. 

Lets start with **word-topic probabilities.**

Every topic is made up of words that are most associated with that topic. Together these words typically form some sort of theme. To understand what this looks like the easiest thing to do is create a bar chart of the top terms in a topic. 

```{r}
ac.topics <- tidy(ac.lda, matrix = "beta")
head(ac.topics)
```
What we have here is a list of topics and the weight of each term in that topic. Essential we have turned this into a one-topic-per-term-per-row format. So, for example, the term 10th has a weight of 5.135047e-05 in topic 1 but 7.269700e-05 in topic 2. Now that doesn't mean a lot to us at this moment and this format is impossible to grasp in its current size and iteration, but we can use tidyverse functions to pair this down and determine the 10 terms that are most common within each topic. 
```{r}
ac.top.terms <- ac.topics %>%
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:5)

ac.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```
(@) Can you adjust the code above to show the top 10 words from just one topic?

```{r}
ac.five.top.terms <- ac.topics %>%
  filter(topic == "5") %>% # Doing a filter here so that it only shows words from topic 5
  arrange(desc(beta)) %>% 
  group_by(topic) %>% slice(1:10) # Getting top 10 terms

ac.five.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

Another useful way to look at the words in each topic is by visualizing them as a wordcloud.
```{r warning=FALSE}
library(wordcloud)
topic1 <- ac.topics %>% filter(topic == 2)
wordcloud(topic1$term, topic1$beta, max.words = 100, random.order = FALSE,
    rot.per = 0.3, colors = brewer.pal(6, "Dark2"))
```


Now we can see what words are most common in each topic. But the document-topic probabilities are also useful for understanding what topics are prevalent in what documents. Just as each topic is made up of a mixture of words, the LDA algorithm also assumes that each topic is made up of a mixture of topics. 

```{r}
ac.documents <- tidy(ac.lda, matrix = "gamma")
head(ac.documents)
```
For each document, the model gives us an estimated proportion of what words in the document are from a topic. So for the April 1915 issue it estimates that about 23% of the words are from topic 1. The gamma number represents the posterior topic distribution for each document. 

This is easier to see if we filter to see the breakdown for just one document. 
```{r}
ac.documents %>%  filter(document == "1916_May.txt") %>% arrange(desc(gamma))
```

This gamma value is really useful and we can use it to see which topics appear in which documents the most. This is frequently referred to as looking at topics over time. 

We can do that using the ac.documents dataframe that we just created but it needs to be joined with the metadata. Again, this is why it is important to have a filename within the metadata spreadsheet. To join these two together we can do a full_join because we want to keep all of the columns.
```{r}
topics.by.year <- full_join(ac.documents, metadata, by = join_by(document == filename))
```

Now what we have is a document that includes the proportion of each topic in each document. Because this is a dataset about a periodical, we have values in our metadata that will make it easy to plot the distrubtion of a topic over time -- in this case for each edition of the journal.

```{r}
topics.by.year$issue_date <- paste(topics.by.year$month, " ", topics.by.year$year, sep = "")
ggplot(data=topics.by.year, aes(x=issue_date, y=gamma)) + geom_bar(stat="identity") + facet_wrap(~ topic, scales = "free") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
```

Using this we can tell that topic 5, which from earlier had the words improve, grow, ties, contracts, and gasoline as the top five words, is most prominent in January 1915. 

(@) Use the rest of this worksheet to experiment with topic modeling. I've added the code to download a much larger dataset - the issues of Mind and Body. This corpus has 413 documents ranging from the 1890s to 1936. You'll want to start with at least 25 topics. 
```{r}
#| eval: false
download.file("https://github.com/regan008/8510-TextAnalysisData/blob/main/MindAndBody.zip?raw=true", "MB.zip")
unzip("MB.zip")
```

```{r}
# Metadata that includes info about each issue.
mb.metadata <- read.csv("https://raw.githubusercontent.com/regan008/8510-TextAnalysisData/main/mb-metadata.csv")
```

```{r}
meta <- as.data.frame(mb.metadata)
meta$Filename <- paste("MB_", meta$Filename, sep="")
file_paths <- system.file("txt/")
mb.texts <- readtext(paste("txt/*.txt", sep=""))
mb.whole <- full_join(meta, mb.texts, by = c("Filename" = "doc_id")) %>% as_tibble()

tidy.mb <- mb.whole %>%
  unnest_tokens(word, text) %>% 
  filter(str_detect(word, "[a-z']$")) %>% 
  anti_join(stop_words)

tidy.mb <- tidy.mb %>% filter(!grepl('[0-9]', word)) 

# Borrowing the code earlier in the worksheet to pull the data and tokenize it + filter for stopwords and numbers.
```

```{r}
tidy.mb.words <- tidy.mb %>%
  count(Filename, word)

mb.dtm <- tidy.mb.words %>%
  count(Filename, word) %>%
  cast_dtm(Filename, word, n)

# Making the DTM
```

```{r}
mb.lda <- LDA(mb.dtm, k = 25, control = list(seed = 54321))
mb.lda
# Training the model\
```

```{r}
mb.topics <- tidy(mb.lda, matrix = "beta")
head(mb.topics)
# Getting the topics
```

```{r}
mb.top.terms <- mb.topics %>%
  arrange(desc(beta)) %>%
  group_by(topic) %>% slice(1:5)

mb.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# Borrowing the code above to make a visualization of the topics.
```

> Now I am going to move on to document-topic probabilities.

```{r}
mb.documents <- tidy(mb.lda, matrix = "gamma")
head(mb.documents)
```

```{r}
mb.metadata <- mb.metadata %>%
  mutate(Filename = paste0("MB_", Filename)) # I had to do this because the documents and Filename columns did not match so I added the MB_ to the metadata so they could match and the full_join can be performed.

mb.topics.by.year <- full_join(mb.documents, mb.metadata, by = join_by(document == Filename))

mb.topics.by.year$Volume <- paste(mb.topics.by.year$Month, " ", topics.by.year$Year, sep = "")
ggplot(data = mb.topics.by.year, aes(x = Volume, y = gamma)) + geom_bar(stat = "identity") +
facet_wrap(~ topic, scales = "free") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

(@) What happens if you create a custom stopword list? How does this change the model?
```{r}
mb.stopwords.custom <- stop_words %>%
  add_row(word = "ulm", lexicon = "custom") %>%
  add_row(word = "rath", lexicon = "custom") %>%
  add_row(word = "wis", lexicon = "custom") %>%
  add_row(word = "pa", lexicon = "custom") %>%
  add_row(word = "st", lexicon = "custom") # I chose to remove some words that just didn't make sense to me.

tidy.mb <- mb.whole %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$")) %>%
  anti_join(mb.stopwords.custom)

tidy.mb <- tidy.mb %>%
  filter(!grepl('[0-9]', word))

tidy.mb.words <- tidy.mb %>% count(Filename, word)

mb.dtm <- tidy.mb.words %>%
  cast_dtm(Filename, word, n)

# Making my dtm
```

```{r}
mb.lda <- LDA(mb.dtm, k = 25, control = list(seed = 32148)) #I made a new seed because I didn't know if it would mess up if I didn't.
```

```{r}
mb.topics <- tidy(mb.lda, matrix = "beta")
head(mb.topics) # Getting my topics now that I have custom stopwords
```

```{r}
mb.top.terms <- mb.topics %>%
  arrange(desc(beta)) %>%
  group_by(topic) %>% slice(1:5)

mb.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

(@) Can you create a topic model for just the documents in the 1920s? How does that change the model? 
```{r}
mb.metadata.twenties <- mb.metadata %>%
  filter(Year >= 1920 & Year < 1930)

mb.twenties <- full_join(mb.metadata.twenties, mb.whole, by = c("Filename" = "doc_id"))

tidy.mb.twenties <- mb.twenties %>%
  unnest_tokens(word, text) %>%
  filter(str_detect(word, "[a-z']$")) %>%
  anti_join(mb.stopwords.custom) %>%
  filter(!grepl('[0-9]', word))

tidy.mb.twenties.words <- tidy.mb.twenties %>%
  count(Filename, word)

mb.dtm.twenties <- tidy.mb.twenties.words %>%
  cast_dtm(Filename, word, n)
```

```{r}
mb.lda.twenties <- LDA(mb.dtm.twenties, k = 25, control = list(seed = 87095))
```

```{r}
mb.twenties.topics <- tidy(mb.lda.twenties, matrix = "beta")
head(mb.twenties.topics) # Getting the topics for the 1920s
```

```{r}
mb.twenties.top.terms <- mb.topics %>%
  arrange(desc(beta)) %>%
  group_by(topic) %>% slice(1:5)

mb.twenties.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

```

(@) Now, choose one of those datasets (from last worksheet), use the same code from last week to download that data, build a topic model about it, and then see what you can say about it historically. You should be able to use topic modeling to address two of the research questions provided:

> I am going to do the Woman Citizen Magazine from last week's WS. First I am going to follow the steps to load the data and metadata and create the dtm and train the lda.

```{r}
download.file("https://github.com/regan008/8510-TextAnalysisData/raw/423e2ec8c0f82b865cdfd09ef0a8ddead51d7292/TheWomanCitizen.zip", "TheWomanCitizen.zip")
unzip("TheWomanCitizen.zip")

wc.metadata <- read.csv("https://github.com/regan008/8510-TextAnalysisData/raw/423e2ec8c0f82b865cdfd09ef0a8ddead51d7292/WomanCitizenMetadata.csv")
```

```{r}
wc.meta <- as.data.frame(wc.metadata)
wc.meta$Filename <- paste("WC_", meta$Filename, sep="")
file_paths <- system.file("TheWomanCitizen/")
wc.texts <- readtext(paste("TheWomanCitizen/*.txt", sep=""))
wc.whole <- full_join(wc.meta, wc.texts, by = c("filename" = "doc_id")) %>% as_tibble() #prob edit the filename thing idk

wc.stop_words_custom <- stop_words %>% add_row(word="women", lexicon="NA") %>% add_row(word="woman", lexicon="NA") %>% add_row(word="avenue", lexicon = "NA") %>% add_row(word="citizen", lexicon = "NA") %>% add_row(word="individual", lexicon = "NA") %>% add_row(word="cents", lexicon = "NA") %>% add_row(word="_d", lexicon = "NA") %>% add_row(word="_i", lexicon = "NA")  %>% add_row(word="_the.same", lexicon = "NA") %>% add_row(word="_w", lexicon = "NA") %>% add_row(word="a'noted", lexicon = "NA") %>% add_row(word="ac", lexicon = "NA") %>% add_row(word="accom", lexicon = "NA") %>% add_row(word="accion", lexicon = "NA") %>% add_row(word="a'noted", lexicon = "NA") %>% add_row(word="aof", lexicon = "NA") %>% add_row(word="ap", lexicon = "NA") %>% add_row(word="ar", lexicon = "NA") %>% add_row(word="ably", lexicon = "NA") %>% add_row(word="ably", lexicon = "NA") %>% add_row(word="ba", lexicon = "NA") %>% add_row(word="abroad", lexicon = "NA") %>% add_row(word="absolutely", lexicon = "NA") %>% add_row(word="accept", lexicon = "NA") %>% add_row(word="accepted", lexicon = "NA") %>% add_row(word="_league", lexicon = "NA") %>% add_row(word="ability", lexicon = "NA") %>% add_row(word="accomplished", lexicon = "NA") %>% add_row(word="active", lexicon = "NA") %>% add_row(word="action", lexicon = "NA") %>% add_row(word="account", lexicon = "NA") %>% add_row(word="acknowledging", lexicon = "NA") %>% add_row(word="actual", lexicon = "NA") %>% add_row(word="achievement", lexicon = "NA") %>% add_row(word="achieved", lexicon = "NA") %>% add_row(word="acting", lexicon = "NA") # Taking the code from the last worksheet where I made custom stopwords for this dataset. I also edited this to add some custom stopwords after running the lda.

tidy.wc <- wc.whole %>%
  unnest_tokens(word, text) %>% 
  filter(str_detect(word, "[a-z']$")) %>% 
  anti_join(wc.stop_words_custom)

tidy.wc <- tidy.wc %>% filter(!grepl('[0-9]', word)) 
```

```{r}
tidy.wc.words <- tidy.wc %>%
  count(filename, word)

wc.dtm <- tidy.wc.words %>%
  count(filename, word) %>%
  cast_dtm(filename, word, n)
```

```{r}
wc.lda <- LDA(wc.dtm, k = 10, control = list(seed = 09071)) # This dataset isn't that big so I just started with ten topics.
wc.lda 
```

> Now I am going to look at the topics we have for this dataset.

```{r}
wc.topics <- tidy(wc.lda, matrix = "beta")
head(wc.topics) # gotta get rid of those bs underscore words. I'm ab to just use a filter probably because I don't want to deal with training a whole other model
```

```{r}
wc.top.terms <- wc.topics %>%
  arrange(desc(beta)) %>%
  group_by(topic) %>% slice(1:5)

wc.top.terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()

# This is awful I gotta fix this with stopwords because wthhh
```

> Question: What do these top topics tell us about this magazines focus?

> Next, I want to see the top topics for every month.

```{r}
wc.documents <- tidy(wc.lda, matrix = "gamma")

wc.topics.by.month <- full_join(wc.documents, wc.metadata, by = c("document" = "filename"))
```

```{r}
wc.topics.by.month <- wc.topics.by.month %>%
  group_by(month, topic) %>%
  summarize(avg_gamma = mean(gamma), .groups = "drop") %>%
  arrange(month, desc(avg_gamma))

wc.topics.by.month %>%
  ggplot(aes(x = month, y = avg_gamma, fill = factor(topic))) +
  geom_col(show.legend = TRUE) +
  labs(title = "Top Topics by Month", x = "Month", y = "Average Gamma", fill = "Topic") +
  theme_minimal()
```

(@) Finally, when you are all done. Write up your findings. What research question did you focus on and what did you learn? 

> 

I focused on trying to figure out which topics were most prominent for each month and compare how the three months varied. I wanted to do this because there is a lot of overlap with these topic groups. I tried multiple times to filter out words and it just kept making more and more overlapping groups. Nevertheless, there were a few unique topics that I suspected may appear in individual months and go along with these overlapping groups. This will give me more information on what specifically each month was focusing on. Each of the "unique" topics had the highest average gamma rating for each month, separately. This helps me determined that this unique topic may have been one of the focuses in the issues from that month.

Outline of findings for the months:

August: 5 (acre, acknowledged, accustomed, abstinence, absence), 7 (addition, added, add, ad, act), 9 (adapted, adams, ad, act, acknowledged)

July: 1 (carnegie, af, kinder, royden, collectively), 2 (adapted, ad, act, acknowledged, accustomed) 3 (bend, department, officials, respect, forms), 8 (admitted, address, addition, added, act), 10 (act, acre, acknowledged, accustomed, absence)

June: 4 (addresses, address, added, ad, act), 6 (adding, added, ad, act, acknowledged), 7 (addition, added, add, ad, act), 8 (admitted, address, addition, added, act)

Given my existing knowledge of the Woman Citizen magazine from the last worksheet, I know that this publication focuses on the suffragist movement and was targeted towards middle and upper-middle class white women. I'll be keeping this in mind as I interpret the topics for each month.

For August, abstinence and absence terms in topic 5 made me think that this topic may have to do with the temperance movement, which overlapped heavily with the suffragist movement. The overlapping topics always seem to be encouraging political action and change, so for August it seems that one of the focuses may have been taking action to encourage abstaining from alcohol. This aligns with demographics of the Women's Temperence movement and the suffragist movement (white middle/upper-middle class).

For July, the terms carnegie, kinder, royden, and collectively stuck out and made me think that this topic could be about individual people. There is also another unique topic here that helps me get an idea of this month's focus— topic 3 includes words like bend, department, officials, respect, and forms. This seems to be about beaurocratic action. With this, it seems that the focus of this month may have been about contacting individuals/representatives and taking beaurocratic action to promote the suffragist movement at the governmental/federal level. 

For June, there aren't as many unique topics here, but there is more mention of addresses in these topics from this month. This makes me think that the issues from June could be focusing on outside or public demonstrations, such as marches, speeches, conferences, etc.

Overall, by looking at how these topics change over each of the three months provided in the corpus, we can see how the Woman's Citizen discussed the suffragist movement over the course of these months. It gives us some historical insight into the different methods that the women's suffragist movement employed for promoting their cause. We see discussion of public demonstrations and addresses, federal and local/state beaurocratic action, and alcohol abstinence/the temperance movement. This shows us how the movement changed overtime to focus on more specific, political action to pursuing more broad, idealistic and moral causes.


